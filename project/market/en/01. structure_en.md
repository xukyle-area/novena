# Tampines Project Interview Preparation - Project Overview & Architecture

## 1. Project Overview

### 1.1 Project Introduction
**Tampines** is a **real-time financial market data processing system** based on Apache Flink, primarily used for processing order books, trade data, and market data.

**Core Value**:

The system delivers significant value through its ability to process massive volumes of market data in real-time with remarkably low latency. It maintains order books with millisecond-level updates while leveraging high-performance stream computing capabilities. The architecture is designed to seamlessly integrate data from multiple cryptocurrency exchanges, providing a unified and scalable solution for financial data processing.

### 1.2 Business Scenarios

This is a **financial trading market data system** designed to serve various critical business needs in the cryptocurrency trading ecosystem. The system continuously maintains exchange order books in real-time, capturing both bid and ask sides to provide accurate market depth information. It aggregates 24-hour trade data to generate comprehensive Ticker information, giving traders insights into daily market movements. Additionally, the system generates candlestick data for different time periods, enabling technical analysis. All this processed data is made available to frontend applications through real-time feeds, with initial support for major cryptocurrency exchanges like Crypto.com, and the architecture allows for easy integration of additional exchanges.

---

## 2. System Architecture

### 2.1 Three-Tier Architecture Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              market-outer (External Interface Layer)      â”‚
â”‚   - WebSocket data ingestion                              â”‚
â”‚   - Spring Boot application                               â”‚
â”‚   - Data writing to Redis/Kafka                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼ (Kafka Topics)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            market-flink (Stream Processing Core Layer)    â”‚
â”‚   - Order Book calculation                                â”‚
â”‚   - Ticker aggregation                                    â”‚
â”‚   - Candle generation                                     â”‚
â”‚   - Flink State management                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼ (Redis)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             market-common (Common Foundation Layer)       â”‚
â”‚   - Data model definitions                                â”‚
â”‚   - Utility classes and constants                         â”‚
â”‚   - Redis client                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Module Details

#### ğŸ“¦ market-common (Common Module)
**Responsibility**: Provide cross-module common components and data models

**Main Contents**:
- **model/**: Data model classes
  - `TradeData`: Trade data
  - `Order`: Order data
  - `OrderBook`: Order book
  - `CurrencyPair`: Trading pair
  - `RealTimeQuote`: Real-time quote
- **redis/**: Redis client wrapper
- **constants/**: Constant definitions
- **enums/**: Enum classes (Market, Side, Action, etc.)
- **exceptions/**: Exception classes
- **utils/**: Utility classes

**Key Dependencies**:
- Redis Client (Jedis)
- Jackson (JSON serialization)
- Guava (utility library)
- Lombok (code simplification)

---

#### ğŸ”„ market-flink (Flink Job Module)
**Responsibility**: Core stream processing logic, implementing real-time data computation

**Main Jobs**:

##### 1ï¸âƒ£ OrderbookJob (Order Book Job)
```java
Kafka[order topic] 
  â†’ KeyBy(contractId)
  â†’ OrderBookProcessor (MapState maintains bids/asks)
  â†’ OrderBookSink (write to Redis)
```

**Core Logic**:
- Uses Flink MapState to maintain quantity at each price level
- Supports INSERT/DELETE operations to update order book
- Supports multi-precision grouping (resolution: 1, 5)
- Triggers output once per second

**State Management**:
```java
// Bid state
MapState<BigDecimal, BigDecimal> bidState;
// Ask state  
MapState<BigDecimal, BigDecimal> askState;
```

##### 2ï¸âƒ£ TickerJob (Market Data Aggregation Job)
```java
Kafka[trade topic]
  â†’ KeyBy(contractId)
  â†’ Window(24h sliding window/1s)
  â†’ TickerAggregator (aggregate trade data)
  â†’ TickerProcessor (generate Ticker)
  â†’ TickerSink (write to Redis)
```

**Core Functions**:
- 24-hour sliding window aggregation
- Calculate high/low prices, volume, etc.
- Updates once per second

##### 3ï¸âƒ£ CandleJob (Candlestick Job)
- Generates candlestick data for different time periods (1min, 5min, 1h, etc.)

**Key Technical Points**:
- Kafka Connector (data source)
- RocksDB State Backend (state storage)
- Event Time & Watermark (time semantics)
- KeyedProcessFunction (core processing logic)

---

#### ğŸŒ market-outer (External Interface Module)
**Responsibility**: Data ingestion layer, responsible for obtaining data from external exchanges

**Main Components**:

**1. WebSocket Client**
```java
// Crypto.com exchange WebSocket client
CryptoSocketClient extends BaseSocketClient
  - Subscribe to market data
  - Heartbeat keep-alive
  - Disconnect reconnection
```

**2. Data Writer**
```java
RedisQuoteWriter
  - Write market data to Redis
  - Provide real-time queries for frontend
```

**3. Spring Boot Application**
- Scheduled task scheduling
- Configuration management
- Health checks

**Key Dependencies**:
- Spring Boot 2.7.18
- Java-WebSocket
- Redis Client

---

## 3. Data Flow Chain

### 3.1 Complete Data Flow

```
External Exchange (Crypto.com)
    â†“ WebSocket
market-outer (Ingestion Layer)
    â†“ 
Redis / Kafka Topics
    â†“
market-flink (Processing Layer)
    â”œâ”€â”€ OrderbookJob â†’ Redis (orderbook:*)
    â”œâ”€â”€ TickerJob â†’ Redis (ticker:*)
    â””â”€â”€ CandleJob â†’ Redis (candle:*)
    â†“
Redis (Storage Layer)
    â†“
Frontend Application / API
```

### 3.2 Kafka Topics Design
Inferred topic design:
- `order` - Order data
- `trade` - Trade data
- `quote` - Market quote data

### 3.3 Redis Key Design
```
orderbook:{resolution}:{contractId}  # Order book
ticker:{contractId}                   # 24-hour market data
candle:{period}:{contractId}          # Candlestick data
```

---

## 4. Core Technical Highlights

### 4.1 Flink State Management

The system leverages Flink's **MapState** for highly efficient order book maintenance, where each price level is stored as a key-value pair enabling O(1) lookup performance. We use **RocksDB** as the state backend, which provides the capability to handle very large state sizes that exceed available memory, making the system suitable for maintaining order books across hundreds of trading pairs simultaneously. An important architectural decision is that each contract maintains completely independent state, which not only prevents any interference between different trading pairs but also allows for easier debugging and state recovery if issues occur.

### 4.2 Price Grouping Algorithm
```java
// Price grouping in OrderBookProcessor
BigDecimal groupedPrice = groupPrice(price, tickSize * resolution);
```

Our price grouping algorithm is a key optimization that significantly reduces memory consumption. Instead of maintaining every individual price point, we merge similar prices into the same level based on a configurable resolution parameter. This approach supports different precision displaysâ€”for example, resolution 1 shows every tick, while resolution 5 or 10 provides aggregated views. This flexibility allows the frontend to request different levels of detail based on user needs without maintaining multiple copies of the data.

### 4.3 Time Window Aggregation

For the Ticker job, we implement a 24-hour sliding window that moves forward one second at a time. This means every second we compute fresh statistics over the last 24 hours of trade data. The sliding window automatically handles the removal of old data and incorporation of new data, providing real-time updates to metrics like high price, low price, and total volume without manual intervention.

### 4.4 High-Concurrency Data Ingestion

The data ingestion layer is built for high concurrency using WebSocket connections that receive data asynchronously from exchanges. All write operations to Redis and Kafka are non-blocking, which means the system never stalls waiting for I/O operations to complete. This asynchronous architecture allows us to handle burst traffic and maintain consistent low latency even under heavy load conditions.

---

## 5. Technology Stack Summary

| Layer                 | Technology Stack    |
| --------------------- | ------------------- |
| **Stream Processing** | Apache Flink 1.17.2 |
| **Message Queue**     | Apache Kafka 3.1.2  |
| **Cache/Storage**     | Redis (Jedis)       |
| **Backend Framework** | Spring Boot 2.7.18  |
| **State Storage**     | RocksDB             |
| **Data Format**       | JSON (Jackson)      |
| **Build Tool**        | Maven               |
| **Java Version**      | Java 8              |
| **Logging Framework** | Logback + SLF4J     |

---

## 6. Deployment Architecture

### 6.1 Component Deployment
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zookeeper  â”‚â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â–¶â”‚    Flink    â”‚
â”‚  :2181      â”‚    â”‚   :9092     â”‚    â”‚   :8081     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis    â”‚â—€â”€â”€â”€â”‚     market-outer (Web)      â”‚
â”‚   :6379     â”‚    â”‚         :10808              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Startup Sequence
1. Zookeeper
2. Kafka
3. Redis
4. Flink Cluster
5. market-flink Jobs (submit to Flink)
6. market-outer (Spring Boot)

---

## 7. Interview Points Summary

### ğŸ¯ When Asked "Tell me about your project":

**Standard Answer Template**:
> "I developed a real-time financial market data processing system based on Apache Flink. This system primarily solves the problem of **real-time processing and order book maintenance** for exchange market data.
> 
> Architecturally, it adopts a **three-tier design**:
> 1. **External interface layer** (market-outer) is responsible for ingesting data from exchanges via WebSocket
> 2. **Stream processing core layer** (market-flink) uses Flink for real-time computation, including order book maintenance, market data aggregation, and candlestick generation
> 3. **Common foundation layer** (market-common) provides shared data models and utility classes
> 
> Technology-wise, it uses **Flink 1.17**, **Kafka 3.1**, and **Redis** as core components. The entire system supports **low-latency** real-time data processing.
>
> I was primarily responsible for **developing Flink jobs**, with OrderbookJob being the most critical. It uses Flink's MapState to maintain bid/ask data for each trading pair and optimizes storage and query efficiency through a price grouping algorithm."

### ğŸ¯ Core Technical Challenges
1. **How to efficiently maintain order books?** â†’ Flink MapState + price grouping
2. **How to ensure data consistency?** â†’ Keyed State + exactly-once semantics
3. **How to handle large state issues?** â†’ RocksDB State Backend
4. **How to ensure low latency?** â†’ Async IO + in-memory computation

---

## 8. Project Value

### 8.1 Business Value
- Supports real-time market data display, enhancing user experience
- High-precision order book maintenance, supporting quantitative trading
- Multi-dimensional data aggregation, supporting data analysis

### 8.2 Technical Value
- Mastery of Flink stream processing framework
- Understanding of financial market microstructure
- Practice in large-scale real-time data processing architecture

### 8.3 Optimization Directions (Bonus Points)
1. Add data deduplication logic (DeduplicateProcessor)
2. Support more exchange integrations
3. Add monitoring and alerting (Flink Metrics)
4. Optimize state storage (Compaction strategy)
5. Support historical data replay (time rollback)

---

## 9. Quick Review Checklist

### âœ… Must Remember Key Points
- [ ] Project name: Tampines
- [ ] Core tech: Flink + Kafka + Redis
- [ ] Three modules: common, flink, outer
- [ ] Three Jobs: OrderbookJob, TickerJob, CandleJob
- [ ] Core state: MapState (order book maintenance)
- [ ] Data source: Kafka, Data sink: Redis
- [ ] Ports: Flink 8081, Redis 6379, Kafka 9092, market-outer 10808

### âœ… Core Concepts
- **OrderBook**: Bid/ask depth data
- **Ticker**: 24-hour market statistics
- **Candle**: Candlestick data
- **Resolution**: Price precision/grouping granularity
- **MapState**: Flink keyed state
- **KeyedProcessFunction**: Processing function

### âœ… Data Flow Direction
External Exchange â†’ WebSocket â†’ market-outer â†’ Kafka â†’ Flink Jobs â†’ Redis â†’ Frontend

---

## 10. Common Interview Questions Preview

### Q1: Why choose Flink?
**A**: 
1. Native support for stream processing, low latency
2. Powerful state management capabilities (MapState suitable for order book maintenance)
3. Exactly-once semantics ensures data consistency
4. Supports large-scale state storage (RocksDB)
5. Mature Kafka Connector

### Q2: How to ensure order book performance?
**A**:
1. Use KeyBy to partition by contractId, each trading pair has independent state
2. Price grouping algorithm reduces state count
3. MapState in-memory operations, O(1) query
4. RocksDB asynchronous writes, non-blocking computation

### Q3: What if order data is delayed or out-of-order?
**A**:
1. Can introduce Watermark to handle out-of-order data
2. Set reasonable allowed lateness
3. OrderBookProcessor already uses ProcessingTime, avoiding out-of-order issues
4. If strict ordering needed, can switch to EventTime + Watermark

### Q4: What if Redis goes down?
**A**:
1. Can configure Redis master-slave + sentinel mode for high availability
2. Flink Sink can configure failure retry strategy
3. Critical data can be written to both Redis and Kafka
4. Use Flink Checkpoint to ensure recoverability

---

**Good luck with your interview! ğŸ‰**
