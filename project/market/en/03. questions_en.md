# Tampines Project Interview Preparation - Common Questions & Answers

## ðŸ“‹ Table of Contents
1. [Project Overview Questions](#1-project-overview-questions)
2. [Technology Selection Questions](#2-technology-selection-questions)
3. [Flink Related Questions](#3-flink-related-questions)
4. [Performance Optimization Questions](#4-performance-optimization-questions)
5. [Failure Handling Questions](#5-failure-handling-questions)
6. [Scalability Questions](#6-scalability-questions)
7. [Practical Scenario Questions](#7-practical-scenario-questions)

---

## 1. Project Overview Questions

### Q1.1: Please introduce your project?
**Standard Answer**:
> This is a real-time financial market data processing system based on Apache Flink, named Tampines. The core objective of the project is to process exchange order book and market data, providing low-latency, high-precision real-time computing capabilities.
>
> **Business Scenario**: The system ingests real-time order and trade data from cryptocurrency exchanges (like Crypto.com) via WebSocket, processes it through Flink stream processing to generate order book depth, 24-hour market data, and candlestick data, storing results in Redis for frontend or quantitative strategy use.
>
> **Technical Architecture**: Uses a three-tier designâ€”external interface layer (market-outer) handles data ingestion, stream processing core layer (market-flink) handles real-time computation, and common foundation layer (market-common) provides shared components. Core technology stack is Flink 1.17, Kafka 3.1, and Redis.
>
> **My Responsibility**: Primarily responsible for Flink job development, including OrderbookJob (order book maintenance), TickerJob (market data aggregation), and CandleJob (candlestick generation), with OrderbookJob being the most critical module, using Flink MapState to achieve high-performance order book maintenance.

**Bonus Points**:
- Emphasize **real-time** (millisecond-level latency)
- Emphasize **precision** (BigDecimal financial calculations)
- Quantify data scale (e.g., processing X thousand orders per second)

---

### Q1.2: What business pain points does this project solve?
**Answer**:

This project addresses several critical challenges in financial market data processing. First, there's the **real-time issue** where traditional batch processing architectures introduce latency measured in minutes, which is completely inadequate for high-frequency trading and real-time market monitoring. By leveraging Flink's stream processing capabilities, we've achieved millisecond-level latency, enabling traders to react to market movements almost instantaneously.

Second, we tackle the **order book maintenance complexity**. Maintaining accurate order books is challenging because they require constant insert and delete operations as orders are placed and filled, and we must ensure precision across potentially hundreds of price levels. Our solution using Flink MapState provides efficient state management that can handle these frequent updates while maintaining consistency.

Third, we address **multi-dimensional data aggregation** requirements. The system needs to simultaneously serve different use casesâ€”some users want highly detailed order books with fine resolution, others prefer aggregated views, and everyone wants 24-hour statistics and multi-period candlestick data. We solve this through Flink's ability to run multiple jobs in parallel, each producing different views of the same underlying data stream.

Finally, there's the **high-concurrency data ingestion** challenge. We're simultaneously receiving data from multiple exchanges covering numerous trading pairs, which can result in tens of thousands of updates per second. Kafka acts as a buffer that decouples our data ingestion layer from processing, providing peak-shaving capabilities and ensuring we never lose data even during traffic spikes.

---

### Q1.3: What was the biggest challenge in the project?
**STAR Method Answer**:

**Situation**: After OrderbookJob had been running in production for a few weeks, we noticed that Flink TaskManager memory usage was continuously growing, and eventually the process would crash with an OutOfMemoryError.

**Task**: My responsibility was to identify the root cause of this memory leak and implement a solution that would allow the job to run stably for extended periods without memory issues.

**Action**: I started by diving into Flink's metrics and State Backend monitoring tools, which revealed that the MapState count was explodingâ€”we had far more price levels being tracked than expected. Through detailed analysis, I discovered that certain trading pairs had extremely dispersed order prices, meaning orders were spread across a very wide price range with small quantities at many different price points. This was causing our state to balloon in size. 

To address this, I implemented a price grouping algorithm using the `groupPrice()` method that intelligently merges similar prices into the same level based on the tick size and a resolution parameter. This dramatically reduced the number of distinct price levels we needed to track. Additionally, I added logic to automatically delete price levels when their quantity dropped to zero, preventing the accumulation of stale state entries. 

As a final optimization, I migrated from HeapStateBackend to RocksDB StateBackend, which stores state on disk with a memory cache, allowing us to handle much larger state sizes without running into memory limits.

**Result**: These changes were transformative. Memory usage dropped by approximately 70%, and the job could now run for a month or more without restart. As a bonus, processing performance actually improved by about 40% because we were maintaining and iterating over far fewer state entries.

**Bonus Points**:
This experience really demonstrated the importance of understanding your data characteristics and not just assuming that default configurations will work. It also showed me how powerful Flink's state management abstractions are when you use them correctly.

---

## 2. Technology Selection Questions

### Q2.1: Why choose Flink over Spark Streaming or Storm?
**Comparison Analysis**:

| Feature                     | Flink                   | Spark Streaming | Storm              |
| --------------------------- | ----------------------- | --------------- | ------------------ |
| **Stream Processing Model** | Native stream           | Micro-batch     | Native stream      |
| **Latency**                 | Millisecond-level       | Second-level    | Millisecond-level  |
| **State Management**        | Powerful (MapState etc) | Weak            | Basic              |
| **Exactly-Once Semantics**  | Supported               | Supported       | At-least-once only |
| **SQL Support**             | Excellent               | Fair            | None               |
| **Ecosystem Maturity**      | High                    | Highest         | Fair               |

**Conclusion**:

**Why we chose Flink**: The decision to use Flink was driven by several key factors that aligned perfectly with our requirements. First and foremost, Flink is a true native stream processing engine, which gives us the low latency we need for real-time order book updatesâ€”something that Spark Streaming's micro-batch model couldn't deliver consistently. 

Second, Flink's MapState abstraction is ideally suited for maintaining the key-value structure of order books, where each price level is a key and the quantity is the value. This made our implementation much more straightforward and performant. 

Third, for a financial application, data consistency is non-negotiable, and Flink's exactly-once processing semantics gives us confidence that we won't lose or double-count any trades or orders. 

Fourth, the RocksDB State Backend allows us to maintain state that's much larger than available memory, which is crucial when you're tracking order books for hundreds of trading pairs. 

Finally, Flink's Kafka Connector is mature and battle-tested, providing reliable integration with our message queue infrastructure. These factors combined made Flink the clear choice over alternatives like Spark Streaming or Storm.

---

### Q2.2: Why use Kafka instead of writing directly to Flink?
**Answer**:

Introducing Kafka between our data ingestion and processing layers provides several critical architectural benefits. 

First, it achieves **decoupling** between market-outer and market-flink, meaning these two components can be deployed, scaled, and upgraded completely independently. This separation of concerns makes our system much more maintainable and reduces deployment risks.

Second, Kafka provides excellent **peak shaving** capabilities. Cryptocurrency markets can experience sudden bursts of activity during major price movements, and Kafka acts as a buffer that absorbs these spikes, preventing our Flink jobs from being overwhelmed. This ensures consistent processing latency even during volatile market conditions.

Third, Kafka enables **data replay**, which is invaluable when debugging issues or recovering from failures. If we discover a bug in our processing logic, we can simply rewind the Kafka consumer offset and reprocess historical data, something that wouldn't be possible with direct connections.

Fourth, the architecture supports **multiple subscriptions**, allowing the same data stream to be consumed by different Flink jobs simultaneously. For example, OrderbookJob, TickerJob, and CandleJob all consume trade data but produce different outputs, and Kafka makes this fan-out pattern straightforward.

Finally, Kafka's **persistence** layer retains data for a configurable period, typically several days, which significantly enhances our fault tolerance and provides a safety net if any component experiences extended downtime.

---

### Q2.3: Why use Redis instead of MySQL to store results?
**Answer**:

The choice of Redis over MySQL for our real-time data storage was driven by several performance and architectural considerations. 

**Performance** is the primary factorâ€”Redis operates entirely in memory and can easily handle over 100,000 queries per second, while MySQL, being disk-based, typically maxes out at a few thousand QPS. When you're serving real-time market data to potentially thousands of concurrent users, this performance difference is critical.

The **data structure alignment** is another key point. Order books are naturally represented as maps where prices are keys and quantities are values. Redis has native support for hash data structures that perfectly match this model, making our code simpler and more efficient. With MySQL, we'd need to design a relational schema that doesn't naturally fit the data.

Redis also provides built-in **expiration mechanisms** through TTL (Time To Live) settings. We can set order book snapshots to automatically expire after a certain period, keeping our dataset fresh without manual cleanup jobs. This is much simpler than writing scheduled tasks to delete old records from MySQL.

For **real-time queries**, latency is crucial. Our frontend might query the order book multiple times per second for each user, and Redis consistently delivers sub-millisecond response times. MySQL, even with proper indexing, would struggle to maintain that level of consistency under load.

That said, we do recognize that historical data analysis is valuable, so we asynchronously write aggregated data to MySQL or ClickHouse for offline analytics. This gives us the best of both worldsâ€”Redis handles the hot, real-time data path, while traditional databases store cold data for historical analysis.

---
### Q2.4: Why use WebSocket for MQTT, and what are the pros and cons compared to native MQTT TCP?
**Answer**:

The choice between MQTT over WebSocket and native MQTT over TCP depends on the deployment environment and integration requirements. In our project, we use WebSocket for market data ingestion from exchanges because many modern exchanges (especially in the crypto space) expose their real-time APIs via WebSocket rather than native MQTT. WebSocket is widely supported by browsers and frontend frameworks, making it ideal for web-based applications and easy integration with JavaScript clients.

**Advantages of MQTT over WebSocket**:
- **Browser Compatibility**: WebSocket is natively supported in browsers, enabling direct market data streaming to web frontends without additional proxies or plugins.
- **Firewall/NAT Traversal**: WebSocket traffic is typically allowed through firewalls and proxies since it uses standard HTTP/HTTPS ports (80/443), whereas native MQTT (often on port 1883/8883) may be blocked in some environments.
- **Unified Protocol Stack**: Using WebSocket for both frontend and backend simplifies the protocol stack and reduces integration complexity.

**Disadvantages**:
- **Performance Overhead**: WebSocket introduces some protocol overhead compared to native MQTT TCP, which can slightly increase latency and reduce throughput in high-frequency scenarios.
- **Feature Limitations**: Some advanced MQTT features (like QoS 2, retained messages, or persistent sessions) may not be fully supported or as efficient over WebSocket as with native MQTT brokers.
- **Ecosystem Maturity**: Native MQTT has a more mature ecosystem for IoT and backend integration, with broader support for device SDKs and broker features.

**Conclusion**: For browser-based or cloud-native applications, MQTT over WebSocket is often the most practical choice due to compatibility and ease of integration. For pure backend or IoT device scenarios, native MQTT TCP may offer better performance and reliability. In our system, since the primary data sources are exchange WebSocket APIs, we use WebSocket for ingestion and only consider MQTT for internal messaging if needed.
---

## 3. Flink Related Questions

### Q3.1: What is Flink's Checkpoint mechanism?
**Answer**:

Checkpoint is the fundamental mechanism that Flink uses to achieve both fault tolerance and exactly-once processing semantics, and it's really quite elegant in its design.

**Working Principle**:

The process begins when the JobManager periodically injects special markers called Checkpoint Barriers into the data stream at the source operators. These barriers flow through the topology along with the regular data. When an operator receives a barrier, it knows it's time to take a snapshot of its current state and save it to durable external storage like HDFS or S3. The barrier continues downstream, triggering each operator in sequence to save its state. Once all operators have confirmed they've completed their snapshots, the checkpoint is considered successful and the JobManager records this.

**Key Configuration** that we use in our project:
```java
see.enableCheckpointing(60000);  // Every 60 seconds
see.getCheckpointConfig()
   .setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);  // Exactly-once
see.setStateBackend(new RocksDBStateBackend("hdfs://..."));  // State storage
```

We checkpoint every 60 seconds, which provides a good balance between overhead and recovery time.

**Recovery Process**:

When something goes wrongâ€”maybe a TaskManager crashes or there's a network partitionâ€”Flink automatically recovers from the most recent successful checkpoint. It restores all operator state to exactly what it was at that checkpoint, and crucially, it also resets the Kafka consumer offsets to their checkpointed positions. This means we'll re-consume and reprocess some data, but because we use exactly-once semantics, this doesn't lead to duplicate results. The system essentially rewinds to that point in time and continues processing as if the failure never happened, guaranteeing we don't lose any data and don't produce incorrect results.

---

### Q3.2: What types of State does Flink have?
**Answer**:

Flink provides two main categories of state, each serving different purposes in stream processing applications.

**Keyed State** is the most commonly used type and must be used after a `keyBy()` operation. The key characteristic is that each key maintains its own independent state, which is automatically managed by Flink. Within Keyed State, there are several specialized types:

**ValueState<T>** stores a single value per key. For example, if you're tracking the last seen ID for deduplication, you might use:
```java
ValueState<Long> lastSeenId;
lastSeenId.update(newId);
```

**MapState<K, V>** is what we heavily rely on in our project. It stores a map structure per key, which is perfect for our order book use case where we need to track multiple price levels (the map keys) and their corresponding quantities (the map values). Here's how we use it:
```java
MapState<BigDecimal, BigDecimal> bidState;
bidState.put(price, quantity);
```
This gives us O(1) access to any price level and makes updates very efficient.

There's also **ListState<T>** for storing lists, **ReducingState<T>** for maintaining cumulative values, and **AggregatingState<IN, OUT>** for more complex aggregations, though we don't use these in our current implementation.

**Operator State** is less common but important for certain scenarios. Unlike Keyed State, it's shared across all parallel instances of an operator. A classic example is Kafka connectors using Operator State to track partition offsets.

**Usage in Our Project**:

OrderBookProcessor is built around MapState to maintain the order book structure efficiently, while TickerAggregator uses Flink's built-in aggregating state through the AggregateFunction interface. This combination gives us both flexibility and performance.

---

### Q3.3: What is Flink's Watermark? Is it used in the project?
**Answer**:

Watermarks are Flink's mechanism for dealing with two common challenges in stream processing: **out-of-order data** and **knowing when to trigger window computations**.

**Concept**:

You can think of a Watermark as a signal that flows through the data stream asserting "all events with timestamps up to time T have now arrived." When a window receives a watermark that exceeds its end time, it knows it's safe to trigger computation because no more late data for that window should be coming. This is crucial when processing events based on their event time rather than the time they're processed.

**Usage in Our Project**:

We actually take different approaches in different jobs. For OrderbookJob, we use Processing Time rather than Event Time, which means we don't need watermarks at all:
```java
see.fromSource(source, WatermarkStrategy.noWatermarks(), KAFKA_SOURCE);
```

However, for TickerJob, which computes 24-hour statistics, we should ideally use Event Time with watermarks to handle potential delays in data arrival:
```java
WatermarkStrategy<Trade> watermarkStrategy = WatermarkStrategy
    .<Trade>forBoundedOutOfOrderness(Duration.ofSeconds(5))
    .withTimestampAssigner((trade, ts) -> trade.getTimestamp());
```

This watermark strategy allows up to 5 seconds of out-of-orderness.

**Why OrderbookJob doesn't use Watermarks**:

For the order book use case, we deliberately chose Processing Time over Event Time for several pragmatic reasons. We use timers that fire every second to output the current state of the order book, and for this use case, the exact event time of each individual order update is less important than maintaining a consistent output cadence. Using Processing Time also significantly simplifies the logic and avoids the complexity of watermark tuning and late data handling. In the fast-moving cryptocurrency market, orders that arrive more than a few seconds late are essentially irrelevant anyway, so Event Time semantics don't provide much additional value for this specific application.

---

### Q3.4: How does Flink ensure Exactly-Once?
**Answer**:

Achieving exactly-once semantics in a distributed streaming system is actually quite challenging, and Flink accomplishes it through careful coordination across three layers: the source, the computation, and the sink.

**At the Source Layer**:

For Kafka sources, which is what we use, Flink takes control of offset management by disabling Kafka's automatic commit feature. Instead, Flink commits offsets as part of the checkpoint process. When a checkpoint completes successfully, Flink records exactly which Kafka offsets were consumed. If there's a failure and we need to recover, we reset to those checkpointed offsets and re-consume the data. This ensures we never skip any messages.

**In the Computation Layer**:

The checkpoint mechanism saves snapshots of all operator state at coordinated points in time. The barrier alignment mechanism ensures that all operators create their snapshots at consistent logical timestamps, preventing any inconsistencies. When we recover from a failure, we restore all operator state from the snapshot and reprocess data from the source, guaranteeing that our computations produce the same results as if the failure had never occurred.

**At the Sink Layer**:

This is where things get interesting because there are two main approaches. The first is **idempotent writes**â€”operations that produce the same result when executed multiple times. Our Redis sink naturally provides this because Redis SET operations are idempotent; writing the same key-value pair twice has the same effect as writing it once. The second approach is **transactional writes** using protocols like Two-Phase Commit, which Kafka sinks can use.

**Implementation in Our Project**:
```java
// Kafka Source doesn't auto-commit Offset
KafkaSource.builder()
    .setProperty("enable.auto.commit", "false")
    .build();

// Redis Sink naturally idempotent (SET overwrites)
redisClient.set(key, value);
```

The beauty of this design is that even if we reprocess data after a failure, our Redis output is correct because the SET operation overwrites any previous value with the latest one, and since we're deterministically recomputing the same value, the end result is exactly what it should be.

---

### Q3.5: How to set Flink's parallelism?
**Answer**:

#### Parallelism Setting Methods (priority from high to low):
1. **Operator Level**:
   ```java
   stream.map(...).setParallelism(4);
   ```

2. **Execution Environment Level**:
   ```java
   see.setParallelism(2);
   ```

3. **Configuration File Level**:
   ```yaml
   # flink-conf.yaml
   parallelism.default: 1
   ```

4. **Job Submission Specification**:
   ```bash
   flink run -p 8 my-job.jar
   ```

**Setting in Project**:
```java
// Globally set to 1 (single parallelism, ensure order)
see.setParallelism(ONE);

// Reasons:
// 1. Order book needs strict Key aggregation, single parallelism avoids partition issues
// 2. Each trading pair computed independently, single parallelism sufficient
// 3. Small data volume (test/dev environment)
```

**Production Environment Optimization**:
- Kafka partitions = Flink parallelism (fully utilize resources)
- Source/Sink high parallelism, ProcessFunction low parallelism
- Dynamically adjust based on monitoring

---

## 4. Performance Optimization Questions

### Q4.1: How to optimize Flink job performance?
**Answer**:

Flink job performance optimization is really a multi-dimensional challenge that requires thinking about several different aspects of the system simultaneously.

**Parallelism Optimization** is often the first lever to pull. The key insight is that not all operators need the same parallelism. Sources and sinks are typically I/O bound, so they benefit from higher parallelism to maximize throughput. Processing operators in the middle might be CPU-bound and need fewer instances. Here's an example:
```java
// Different parallelism for different operators
source.setParallelism(4)      // Source high parallelism for reading
      .keyBy(...)
      .process(...).setParallelism(2)  // Moderate computation
      .addSink(...).setParallelism(4);  // Sink high parallelism for writing
```

**State Optimization** becomes critical as your state grows. We use RocksDB StateBackend which stores state on disk with a memory cache, allowing us to handle state sizes that far exceed available RAM. Enabling incremental checkpoints means we only need to snapshot the state changes since the last checkpoint, dramatically reducing checkpoint overhead. Additionally, configuring State TTL (Time To Live) automatically removes old state entries:
```java
see.setStateBackend(new RocksDBStateBackend("hdfs://..."));
see.getCheckpointConfig().enableIncrementalCheckpoints(true);

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.days(1))
    .setUpdateType(UpdateType.OnCreateAndWrite)
    .build();
descriptor.enableTimeToLive(ttlConfig);
```

**Network Optimization** involves tuning buffer timeout settings. The network buffer timeout controls how long Flink waits before flushing output buffers. Lower values reduce latency but increase overhead; higher values improve throughput but add latency. We use 100ms as a reasonable balance:
```java
see.getConfig().setNetworkBufferTimeout(100);  // 100ms
```

**Operator Chain Optimization** can be useful during debugging. By default, Flink chains operators together to minimize serialization overhead, but sometimes you want to break these chains to better understand where bottlenecks occur:
```java
see.disableOperatorChaining();  // For debugging
stream.map(...).startNewChain();  // Force break at specific point
```

**Data Skew Handling** is crucial when some keys get much more traffic than others. A common technique is adding random prefixes before the keyBy operation to distribute hot keys across multiple partitions, then removing the prefix for final aggregation:
```java
.map(data -> {
    int random = ThreadLocalRandom.current().nextInt(10);
    return Tuple2.of(random + "_" + data.getKey(), data);
})
.keyBy(0)
```

**In Our Project**, we primarily focus on three optimizations: the price grouping algorithm reduces state size by consolidating similar prices; automatically deleting price levels when quantity reaches zero prevents state accumulation; and using RocksDB StateBackend allows us to handle much larger state than would fit in memory. Future improvements could include implementing async Redis writes and incremental order book outputs to further improve performance.

---

### Q4.2: How to handle data skew?
**Answer**:

**Identifying Data Skew**:
- Check Flink Web UI for SubTask processed record counts
- Some SubTasks process far more than others

**Solutions**:

#### Solution 1: Add Random Prefix Before KeyBy
```java
// Two-stage aggregation
// First stage: Local aggregation
stream.map(data -> {
    int salt = random.nextInt(10);
    return Tuple2.of(salt + "_" + data.getKey(), data);
})
.keyBy(0)
.reduce(...)

// Second stage: Global aggregation
.map(data -> data.f1)  // Remove random prefix
.keyBy(...)
.reduce(...)
```

#### Solution 2: Custom Partitioner
```java
stream.partitionCustom(new Partitioner<String>() {
    @Override
    public int partition(String key, int numPartitions) {
        // Special handling for hot keys
        if (isHotKey(key)) {
            return ThreadLocalRandom.current().nextInt(numPartitions);
        }
        return key.hashCode() % numPartitions;
    }
}, KeySelector);
```

#### Solution 3: Filter Hot Data
```java
// Process hot trading pairs separately
stream.split(data -> {
    if (isHotContract(data.getContractId())) {
        return "hot";
    }
    return "normal";
})
.select("hot").setParallelism(8)   // Hot data high parallelism
.union(
    stream.select("normal").setParallelism(2)  // Normal data low parallelism
);
```

---

### Q4.3: How to monitor Flink jobs?
**Answer**:

#### 1. Flink Web UI (port 8081)
- **Overview**: Overall running status, TaskManager count
- **Jobs**: Job list, running time
- **Task Metrics**: Each operator's processing speed, Backpressure
- **Checkpoints**: Success/failure count, duration

#### 2. Flink Metrics
```java
public class OrderBookProcessor 
    extends KeyedProcessFunction<Long, Order, OrderBook> {
    
    private transient Counter processedOrders;
    private transient Meter processingRate;

    @Override
    public void open(Configuration parameters) {
        MetricGroup metricGroup = getRuntimeContext().getMetricGroup();
        
        // Counter
        processedOrders = metricGroup.counter("processed_orders");
        
        // Rate
        processingRate = metricGroup.meter("processing_rate", 
            new MeterView(60));  // 60-second window
        
        // Distribution statistics
        metricGroup.gauge("orderbook_size", () -> bidState.keys().iterator().hasNext() ? 1 : 0);
    }

    @Override
    public void processElement(...) {
        processedOrders.inc();
        processingRate.markEvent();
        // ...
    }
}
```

#### 3. External Monitoring Systems
- **Prometheus + Grafana**: Collect Flink Metrics, visual display
- **ELK**: Collect logs, analyze exceptions
- **AlertManager**: Alert notifications

**Key Metrics**:
- **Throughput**: records/s
- **Latency**: end-to-end latency
- **Backpressure**: 0-1, close to 1 indicates high pressure
- **Checkpoint Duration**: Checkpoint time cost
- **State Size**: State size

---

## 5. Failure Handling Questions

### Q5.1: What if Flink job crashes?
**Answer**:

#### Automatic Recovery
```java
// Configure restart strategy
see.setRestartStrategy(RestartStrategies.failureRateRestart(
    3,                              // Max failure count
    Time.of(5, TimeUnit.MINUTES),   // Time window
    Time.of(10, TimeUnit.SECONDS)   // Restart interval
));
```

**Restart Process**:
1. Flink detects TaskManager failure
2. Cancels all Tasks
3. Recovers state from most recent Checkpoint
4. Re-submits Tasks
5. Kafka Source continues consuming from Checkpoint's recorded Offset

#### Common Failure Causes
1. **OOM**: Increase memory, optimize state
2. **Network Exception**: Configure retry, timeout
3. **External Dependency Unavailable** (e.g., Redis): Health check, degradation
4. **Data Exception**: Add exception handling, filter dirty data

---

### Q5.2: What if Redis is unavailable?
**Answer**:

#### Solution 1: Retry Mechanism
```java
public class OrderBookSink extends RichSinkFunction<OrderBook> {
    private static final int MAX_RETRIES = 3;

    @Override
    public void invoke(OrderBook orderBook, Context context) {
        for (int i = 0; i < MAX_RETRIES; i++) {
            try {
                redisClient.set(key, value);
                return;  // Return on success
            } catch (Exception e) {
                if (i == MAX_RETRIES - 1) {
                    throw e;  // Throw on last failure
                }
                Thread.sleep(1000 * (i + 1));  // Exponential backoff
            }
        }
    }
}
```

#### Solution 2: Degradation Strategy
```java
if (!redisClient.isAvailable()) {
    // Degradation: write to local file
    localFileWriter.write(orderBook);
    // Later async compensation
}
```

#### Solution 3: Dual Write
```java
// Write to both Redis and Kafka
redisClient.set(key, value);
kafkaProducer.send("result_topic", value);  // Backup
```

#### Solution 4: Redis High Availability
- Redis Sentinel: Automatic failover
- Redis Cluster: Sharding + high availability

---

### Q5.3: What if Kafka messages accumulate?
**Answer**:

#### 1. Temporary Scale-up
```bash
# Increase Flink parallelism
flink run -p 8 my-job.jar  # From 4 to 8
```

#### 2. Optimize Code
- Reduce recomputation
- Async IO instead of sync IO
- Batch writes instead of single writes

#### 3. Increase Kafka Partitions
```bash
kafka-topics.sh --alter --topic order --partitions 16
```
**Note**: Flink parallelism should also increase accordingly

#### 4. Rate Limiting
```java
// Rate limit operators with downstream pressure
stream.rescale().setParallelism(2);  // Reduce parallelism
```

#### 5. Temporarily Stop Low-Priority Jobs
- Prioritize core Jobs (e.g., OrderbookJob)
- Pause secondary Jobs (e.g., CandleJob)

---

### Q5.4: How to handle dirty data?
**Answer**:

#### Solution 1: Filter Invalid Data
```java
stream.filter(order -> {
    if (order.getPrice() == null || order.getPrice().compareTo(BigDecimal.ZERO) <= 0) {
        log.warn("Invalid order: {}", order);
        return false;
    }
    return true;
});
```

#### Solution 2: Exception Catching
```java
@Override
public void processElement(Order order, Context ctx, Collector<OrderBook> out) {
    try {
        // Processing logic
    } catch (Exception e) {
        log.error("Process failed: {}", order, e);
        // Record to side output stream
        ctx.output(errorTag, order);
    }
}
```

#### Solution 3: Side Output Stream
```java
// Define side output tag
OutputTag<Order> errorTag = new OutputTag<Order>("error-orders") {};

// Main stream processes normally, exception data output to side stream
DataStream<OrderBook> mainStream = ...
DataStream<Order> errorStream = mainStream.getSideOutput(errorTag);

// Error data handled separately (e.g., log, alert)
errorStream.addSink(new ErrorLogSink());
```

---

## 6. Scalability Questions

### Q6.1: How to extend if supporting new exchange?
**Answer**:

#### 1. Add New WebSocket Client
```java
// Add BinanceSocketClient
public class BinanceSocketClient extends BaseSocketClient {
    @Override
    public void onMessage(String message) {
        // Parse Binance-specific format
        BinanceEvent event = parseMessage(message);
        
        // Convert to unified model
        Order order = convertToOrder(event);
        
        // Write to Kafka
        kafkaProducer.send("order", order);
    }
}
```

#### 2. Unified Data Model
```java
// Define unified model in market-common
public class Order {
    private Long contractId;
    private String exchange;  // New: exchange identifier
    // ...
}
```

#### 3. Configuration Management
```yaml
# application.yml
exchanges:
  - name: crypto.com
    websocket: wss://stream.crypto.com/v2/market
    symbols: [BTC_USDT, ETH_USDT]
  - name: binance
    websocket: wss://stream.binance.com:9443/ws
    symbols: [BTCUSDT, ETHUSDT]
```

#### 4. Flink Jobs No Modification Needed
- As long as data format is unified, Flink jobs transparently process
- Distinguish data source through `exchange` field

---

### Q6.2: How to support more trading pairs?
**Answer**:

#### 1. Dynamic Configuration
```java
// Read trading pair list from config file or database
List<String> symbols = configService.getSymbols();

// Dynamic subscription
for (String symbol : symbols) {
    socketClient.subscribe("book." + symbol);
}
```

#### 2. Automatic Mapping
```java
// ContractId mapping table
public class ContractIdMapper {
    private static Map<String, Long> symbolToIdMap = new ConcurrentHashMap<>();
    
    static {
        symbolToIdMap.put("BTC_USDT", 1L);
        symbolToIdMap.put("ETH_USDT", 2L);
        // ... can load from database
    }
    
    public static Long getContractId(String symbol) {
        return symbolToIdMap.computeIfAbsent(symbol, 
            s -> allocateNewId(s));  // Dynamic allocation
    }
}
```

#### 3. Flink State Auto Expansion
- KeyBy(contractId) naturally supports new trading pairs
- New Keys automatically create independent state
- No need to restart job

---

### Q6.3: How to support historical data replay?
**Answer**:

#### Solution 1: Kafka Time Rollback
```bash
# Reset consumption position to specified time
kafka-consumer-groups.sh --reset-offsets \
  --group orderbook \
  --topic order \
  --to-datetime 2024-01-01T00:00:00.000 \
  --execute
```

#### Solution 2: Read from File
```java
// Replace Kafka Source with file Source
StreamExecutionEnvironment see = ...;
DataStream<Order> stream = see.readTextFile("hdfs://orders.log")
    .map(line -> JSON.parseObject(line, Order.class));
```

#### Solution 3: SavePoint + Time Rollback
```bash
# 1. Recover from SavePoint
flink run -s hdfs://savepoints/savepoint-123 my-job.jar

# 2. Kafka Offset reset
kafka-consumer-groups.sh --reset-offsets ...
```

---

## 7. Practical Scenario Questions

### Q7.1: If order book suddenly stops updating, how to troubleshoot?
**Troubleshooting Process**:

#### Step 1: Confirm Phenomenon
```bash
# Check latest data timestamp in Redis
redis-cli
> GET orderbook:1:1
> # Check timestamp field
```

#### Step 2: Check Flink Job Status
```bash
# Access Flink Web UI (http://localhost:8081)
# Check if OrderbookJob is RUNNING
```

#### Step 3: Check Logs
```bash
# TaskManager logs
tail -f /usr/local/flink/log/flink-*-taskexecutor-*.log

# Find exception stacks
grep "ERROR" flink-*-taskexecutor-*.log
```

#### Step 4: Check Kafka Consumption
```bash
# Check consumption Lag
kafka-consumer-groups.sh --describe --group orderbook

# If Lag is 0, no new data
# If Lag is large, consumption blocked
```

#### Step 5: Check market-outer
```bash
# Check market-outer logs
tail -f /path/to/market-outer.log

# Check WebSocket connection status
# Check if writing to Kafka normally
```

#### Step 6: Check Dependent Services
```bash
# Redis
redis-cli PING

# Kafka
kafka-broker-api-versions.sh --bootstrap-server localhost:9092

# Zookeeper
echo stat | nc localhost 2181
```

**Common Causes**:
1. Flink job Checkpoint timeout restart
2. Kafka connection disconnected
3. WebSocket disconnected without reconnection
4. Redis write failure (disk full)
5. Network partition

---

### Q7.2: If need to improve throughput by 10x, how to redesign?
**Answer**:

#### 1. Architecture Level
- **Increase Kafka Partitions**: 16 â†’ 160
- **Increase Flink Parallelism**: 1 â†’ 32
- **Increase TaskManagers**: 1 â†’ 8 servers

#### 2. Code Level
```java
// Async Redis write
AsyncDataStream.unorderedWait(
    stream,
    new AsyncRedisFunction(),
    1000,  // Timeout
    TimeUnit.MILLISECONDS,
    100    // Async request capacity
);
```

#### 3. State Storage
- RocksDB configuration optimization:
```java
RocksDBOptionsFactory factory = new RocksDBOptionsFactory() {
    @Override
    public DBOptions createDBOptions(DBOptions currentOptions) {
        return currentOptions
            .setIncreaseParallelism(4)
            .setMaxBackgroundJobs(4);
    }
    
    @Override
    public ColumnFamilyOptions createColumnOptions(ColumnFamilyOptions currentOptions) {
        return currentOptions
            .setCompactionStyle(CompactionStyle.LEVEL)
            .setWriteBufferSize(64 * 1024 * 1024);  // 64MB
    }
};
see.setStateBackend(new RocksDBStateBackend("hdfs://...", true));
```

#### 4. Network Optimization
```java
see.getConfig().setNetworkBufferTimeout(10);  // 10ms quick flush
```

#### 5. Data Optimization
- Order merging: batch process orders with same price
- Timer optimization: 1s â†’ 100ms
- Incremental updates: only send changed price levels

**Expected Improvement**:
- Single machine 10k TPS â†’ Distributed cluster 100k TPS

---

### Q7.3: How to design to ensure 100% order book accuracy?
**Answer**:

#### 1. Data Source Side
- WebSocket heartbeat keep-alive, immediate reconnection on disconnect
- Add order sequence number validation, detect missing
```java
if (order.getSeqId() != lastSeqId + 1) {
    log.error("Missing orders: {} to {}", lastSeqId + 1, order.getSeqId() - 1);
    // Trigger full sync
    requestFullSnapshot();
}
```

#### 2. Transmission Side
- Kafka configured with `acks=all`, ensure no message loss
- Flink configured with Exactly-Once semantics
- Redis persistence (AOF mode)

#### 3. Computation Side
```java
// Add validation logic
@Override
public void onTimer(...) {
    OrderBook orderBook = buildOrderBook();
    
    // Validate order book legality
    if (!validateOrderBook(orderBook)) {
        log.error("Invalid orderbook: {}", orderBook);
        // Trigger alert
        alertService.send("OrderBook validation failed");
        // Don't output error data
        return;
    }
    
    out.collect(orderBook);
}

private boolean validateOrderBook(OrderBook book) {
    // 1. Best bid < best ask
    BigDecimal bestBid = book.getBids().keySet().stream().max(Comparator.naturalOrder()).orElse(BigDecimal.ZERO);
    BigDecimal bestAsk = book.getAsks().keySet().stream().min(Comparator.naturalOrder()).orElse(BigDecimal.ZERO);
    if (bestBid.compareTo(bestAsk) >= 0) {
        return false;
    }
    
    // 2. All quantities > 0
    for (BigDecimal qty : book.getBids().values()) {
        if (qty.compareTo(BigDecimal.ZERO) <= 0) {
            return false;
        }
    }
    
    return true;
}
```

#### 4. Reconciliation Mechanism
```java
// Scheduled full reconciliation
@Scheduled(cron = "0 0 * * * ?")  // Every hour
public void reconcile() {
    // Get full order book from exchange API
    OrderBook fullSnapshot = exchangeApi.getFullOrderBook(contractId);
    
    // Compare with data in Redis
    OrderBook cachedBook = redisClient.get("orderbook:1:1");
    
    if (!fullSnapshot.equals(cachedBook)) {
        log.error("OrderBook mismatch, triggering full sync");
        // Full sync
        redisClient.set("orderbook:1:1", fullSnapshot);
    }
}
```

---

## 8. Behavioral Interview Questions

### Q8.1: How to handle disagreements in teamwork?
**STAR Answer**:
- **Situation**: When developing OrderbookJob, team had disagreement on price grouping algorithm implementation.
- **Task**: Needed to find balance between precision and performance.
- **Action**:
  1. Organized technical discussion meeting, listed pros/cons of each approach
  2. Set up test environment, compared performance data of different approaches
  3. Finally adopted dynamic resolution approach, ensuring both precision and state size control
  4. Formed technical documentation, reached consensus
- **Result**: Approach ran stably after launch, team approved

---

### Q8.2: How do you learn new technologies?
**Answer (Using Flink as Example)**:
1. **Official Documentation**: Read Flink official docs, understand core concepts
2. **Hands-on Practice**: Set up local environment, run example programs
3. **Source Code Reading**: Study implementation of key classes (e.g., KeyedProcessFunction)
4. **Community Exchange**: Participate in technical forums (e.g., Apache Flink Chinese community)
5. **Practical Projects**: Apply in Tampines project, solve real problems
6. **Summary Output**: Write technical blogs, deepen understanding

---

### Q8.3: What are your career plans?
**Example Answer**:
> Short-term (1-2 years): Deep dive into real-time computing, become Flink technical expert, able to independently design and optimize large-scale stream processing systems.
>
> Mid-term (3-5 years): Expand to big data architecture, master data warehouse, data lake, OLAP technologies, become technical leader.
>
> Long-term (5+ years): Move toward architect or technical management direction, able to plan entire company's data platform architecture.

---

## 9. Quick Review Checklist

### âœ… Must-Know Points
- [ ] Project background and business value
- [ ] Three-tier architecture design
- [ ] OrderBookProcessor core logic
- [ ] Flink State types and usage
- [ ] Checkpoint mechanism
- [ ] Exactly-Once semantics implementation
- [ ] Performance optimization methods (5+)
- [ ] Failure troubleshooting process

### âœ… High-Frequency Questions (Focus Preparation)
1. Introduce project
2. Why choose Flink
3. How to maintain order book
4. How to ensure data consistency
5. Technical challenges encountered
6. Performance optimization methods
7. Failure handling experience

### âœ… Data Metrics (Recommend Memorizing)
- Flink version: 1.17.2
- Kafka version: 3.1.2
- Java version: 1.8
- Parallelism: 1 (scalable to 32+)
- Checkpoint interval: 60 seconds
- Order book precision: 1/5/10/100
- Window size: 24-hour sliding window

---

**Wishing you interview success! ðŸ’ª**
