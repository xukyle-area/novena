# Tampines Project Interview Preparation - Core Code Analysis

## 1. OrderBookProcessor Deep Dive

### 1.1 Complete Flow Diagram
```
Order Flow In (Order)
    ‚Üì
KeyBy(contractId)  # Group by trading pair
    ‚Üì
processElement()   # Process each order
    ‚îú‚îÄ Price grouping (groupPrice)
    ‚îú‚îÄ Update MapState
    ‚îÇ   ‚îú‚îÄ INSERT: Add quantity
    ‚îÇ   ‚îî‚îÄ DELETE: Reduce/remove
    ‚îî‚îÄ Register timer
    ‚Üì
onTimer() # Triggered every second
    ‚îú‚îÄ Iterate bidState/askState
    ‚îú‚îÄ Build OrderBook object
    ‚îî‚îÄ Output to downstream (Sink)
    ‚Üì
OrderBookSink
    ‚îî‚îÄ Write to Redis
```

### 1.2 Core Code Line-by-Line Analysis

#### üìå State Initialization
```java
private MapState<BigDecimal, BigDecimal> bidState;
private MapState<BigDecimal, BigDecimal> askState;

@Override
public void open(Configuration parameters) throws Exception {
    // Initialize bid state (Key: price, Value: quantity)
    bidState = getRuntimeContext()
            .getMapState(new MapStateDescriptor<>(
                Side.BID.name(), 
                BigDecimal.class, 
                BigDecimal.class));
    
    // Initialize ask state
    askState = getRuntimeContext()
            .getMapState(new MapStateDescriptor<>(
                Side.ASK.name(), 
                BigDecimal.class, 
                BigDecimal.class));
}
```

**Interview Points**:

When explaining this code in an interview, emphasize that `MapState` is one of Flink's keyed state types specifically designed for storing key-value pairs efficiently. An important characteristic is that each contractId (our key) maintains its own completely independent MapState instance, which Flink manages automatically. We deliberately chose `BigDecimal` over primitive types like double because financial calculations demand absolute precision‚Äîeven tiny rounding errors are unacceptable when dealing with money. The `open()` method is part of Flink's RichFunction lifecycle and is called exactly once when the operator is initialized, making it the perfect place to set up our state structures.

---

#### üìå Order Processing Logic
```java
@Override
public void processElement(Order order, Context ctx, Collector<OrderBook> out) 
    throws Exception {
    
    // 1. Get contract info, calculate grouping granularity
    Contract contract = Contract.getContractById(order.getContractId());
    this.grouping = contract.getTickSize() * resolution;
    
    // 2. Select bid or ask state
    MapState<BigDecimal, BigDecimal> sideState = 
        Side.BID.name().equals(order.getSide()) ? bidState : askState;

    // 3. Price grouping
    BigDecimal price = order.getPrice();
    BigDecimal groupedPrice = this.groupPrice(price, this.grouping);
    BigDecimal quantity = order.getQuantity();
    BigDecimal currentQuantity = sideState.get(groupedPrice);

    // 4. Update state based on action type
    if (Action.INSERT.name().equals(order.getAction())) {
        // New order: add quantity
        BigDecimal newQuantity = 
            currentQuantity != null ? currentQuantity.add(quantity) : quantity;
        sideState.put(groupedPrice, newQuantity);
        
    } else if (Action.DELETE.name().equals(order.getAction())) {
        // Delete order: reduce quantity
        if (currentQuantity != null) {
            BigDecimal newQuantity = currentQuantity.subtract(quantity);
            if (newQuantity.compareTo(BigDecimal.ZERO) <= 0) {
                sideState.remove(groupedPrice);  // Remove if quantity is 0
            } else {
                sideState.put(groupedPrice, newQuantity);
            }
        }
    }

    // 5. Register timer, trigger once per second
    long nextTimer = ctx.timerService().currentProcessingTime() / 1000 * 1000 + 1000;
    ctx.timerService().registerProcessingTimeTimer(nextTimer);
}
```

**Interview Points**:

There are several sophisticated aspects of this order processing logic worth highlighting. The **price grouping** mechanism is a key optimization‚Äîinstead of tracking every individual price point, we merge similar prices into groups based on the tick size and resolution. This dramatically reduces the number of state entries we need to maintain, which is crucial for performance and memory usage.

The logic is designed to be **idempotent** in the sense that if we receive the same INSERT operation multiple times, the quantities will accumulate correctly, and multiple DELETE operations will progressively reduce the quantity. This property is important for handling potential duplicates in the data stream.

We implement **automatic state cleanup** by removing price levels when their quantity drops to zero or below. This is essential for preventing the state from growing indefinitely over time, especially for volatile markets where price levels come and go frequently.

The **timer registration** at the end deserves special mention. We use processing time timers that fire at aligned boundaries‚Äîat exactly 1000ms, 2000ms, 3000ms, and so on. This alignment ensures that all trading pairs output their order books at the same moment, which simplifies downstream processing and provides a consistent view of the market. It also means we're not bombarding Redis with updates on every single order change, but instead batching updates into one-second intervals.

---

#### üìå Timer Trigger
```java
@Override
public void onTimer(long timestamp, OnTimerContext ctx, Collector<OrderBook> out) 
    throws Exception {
    
    OrderBook orderBook = new OrderBook();

    // 1. Iterate bid state
    for (Map.Entry<BigDecimal, BigDecimal> entry : bidState.entries()) {
        orderBook.getBids().put(entry.getKey(), entry.getValue());
    }
    
    // 2. Iterate ask state
    for (Map.Entry<BigDecimal, BigDecimal> entry : askState.entries()) {
        orderBook.getAsks().put(entry.getKey(), entry.getValue());
    }
    
    // 3. Set metadata
    orderBook.setMarket(Market.GANTEN);
    orderBook.setContractId(ctx.getCurrentKey());  // Current Key (contractId)
    orderBook.setGrouping(this.grouping);
    
    // 4. Output to downstream
    out.collect(orderBook);
}
```

**Interview Points**:

The onTimer method is elegantly simple but serves a critical function. Every second, it generates a complete snapshot of the current order book state by iterating through all the price levels we've been maintaining in our MapState. The `entries()` method on MapState provides an iterator over all key-value pairs, allowing us to efficiently copy them into our OrderBook data structure.

An interesting detail is how we use `getCurrentKey()` to retrieve the contractId we're currently processing. Remember that because we keyed the stream by contractId, Flink automatically handles the context switching for us‚Äîwhen this timer fires, we're always in the context of a specific contract, and getCurrentKey tells us which one.

Finally, we output the complete OrderBook to the `Collector`, which flows it downstream to our Redis sink. This design means Redis always receives complete snapshots rather than incremental updates, which simplifies the consumer logic at the cost of slightly higher bandwidth. In practice, order books for most trading pairs are small enough that this tradeoff is very favorable.

---

#### üìå Price Grouping Algorithm
```java
private BigDecimal groupPrice(BigDecimal price, double grouping) {
    // 1. Calculate which group the price belongs to
    // Example: price=100.23, grouping=5
    //     Group number = floor(100.23 / 5) = 20
    //     Group price = 20 * 5 = 100.0
    
    BigDecimal groupingBD = BigDecimal.valueOf(grouping);
    return price.divide(groupingBD, 0, RoundingMode.DOWN)
                .multiply(groupingBD);
}
```

**Example Calculation**:
```
resolution = 5
tickSize = 0.5
grouping = 5 * 0.5 = 2.5

Price 100.1 ‚Üí Group 100.0
Price 100.8 ‚Üí Group 100.0
Price 102.6 ‚Üí Group 102.5
Price 105.0 ‚Üí Group 105.0
```

**Interview Points**:

The price grouping algorithm is mathematically straightforward but has profound implications for system performance. We use `RoundingMode.DOWN` to ensure we always round prices down to the nearest group boundary, which provides consistent and predictable behavior.

The real value here is in how this reduces our state storage requirements. Instead of potentially tracking thousands of individual price points, we might only need to track dozens of price groups. The resolution parameter provides flexibility‚Äîsetting it to 1 gives you every tick (maximum precision), while higher resolutions like 5 or 10 provide progressively coarser views that are perfectly adequate for many use cases and dramatically reduce memory usage.

This also enables an elegant user experience where the frontend can request different resolution views of the same underlying data without requiring us to maintain multiple copies of the state. A retail trader might want resolution 5 for a clean view, while a market maker might request resolution 1 for maximum granularity.

---

### 1.3 State Management Key Questions

#### ‚ùì Q: Where is MapState stored?
**A**: 

The storage location of MapState actually depends on the State Backend configuration you choose. By default, Flink uses the HeapStateBackend, which stores all state directly in the **TaskManager's JVM heap memory**. This provides excellent performance for small to medium-sized state, but you're constrained by available memory.

For larger state requirements, we configure RocksDB as our State Backend, which stores state on **local disk** with a memory cache layer. This allows us to handle state sizes that far exceed available RAM, though at a slight performance cost due to disk I/O. The beauty of RocksDB is that it's completely transparent to your application code\u2014the same MapState API works regardless of the backend.

Regardless of which backend you use, state is also periodically **persisted to external storage** like HDFS or S3 through Flink's checkpoint mechanism. These checkpoints serve as recovery points if something goes wrong, ensuring you never lose your state even if entire TaskManagers fail.

#### ‚ùì Q: How to ensure state isolation for different Keys?
**A**: 

Flink's keyed state mechanism provides automatic isolation, and it's quite elegant. When you call `keyBy(Order::getContractId)`, Flink partitions your data stream so that all orders for a given contractId are always routed to the same parallel operator instance. Within that instance, Flink maintains a separate MapState for each unique key value.

When your code calls methods like `bidState.get(price)` or `bidState.put(price, quantity)`, Flink automatically scopes these operations to the current key's state. The framework handles all the context switching for you\u2014you write your code as if you're only dealing with one contract at a time, but Flink is actually multiplexing state for potentially hundreds of contracts within the same operator instance.

This isolation means that if there's a problem with the state for one trading pair, it doesn't affect any others. It also makes debugging much easier because you can reason about each contract independently.

#### ‚ùì Q: Will state grow infinitely?
**A**:

Infinite state growth is a legitimate concern in any stateful stream processing application, and we've taken several measures to address it. The primary mechanism in our project is **explicit state cleanup**\u2014whenever a price level's quantity drops to zero, we actively delete that entry from the MapState using the `remove()` method. This is crucial because in a dynamic market, price levels are constantly appearing and disappearing, and we don't want to keep memory allocated for levels that no longer have any orders.

For additional safety, Flink provides a feature called **State TTL (Time To Live)** that can automatically expire state entries that haven't been updated within a specified time period. While we don't currently use this for order books (since we want to maintain all active price levels), it's valuable for other types of state where you know data becomes stale after a certain time.

Finally, using **RocksDB with incremental checkpoints** helps manage the checkpoint overhead as state grows. Instead of copying the entire state on every checkpoint, RocksDB only persists the changes since the last checkpoint, making the process much more efficient even with large state sizes.

---

## 2. TickerJob Deep Dive

### 2.1 Core Logic
```java
KeyedStream<Trade, Long> keyedStream = tradeStream.keyBy(Trade::getContractId);

keyedStream
    // 1. Define window: 24-hour window, sliding every second
    .window(SlidingEventTimeWindows.of(Time.hours(24), Time.seconds(1)))
    
    // 2. Aggregate function: accumulate trade data
    .aggregate(new TickerAggregator(), new TickerProcessor())
    
    // 3. Output to Redis
    .addSink(new TickerSink());
```

### 2.2 Sliding Window Principle

```
Timeline: [00:00 ----------- 24:00 ----------- 48:00]

Window1: [00:00 ~ 24:00]  # Second 1
Window2: [00:01 ~ 24:01]  # Second 2
Window3: [00:02 ~ 24:02]  # Second 3
...
```

**Characteristics**:
- Triggers computation once per second
- Window size 24 hours, always computes last 24 hours of data
- Automatically removes data older than 24 hours

### 2.3 Aggregate Function (Inferred Implementation)
```java
public class TickerAggregator 
    implements AggregateFunction<Trade, TickerAccumulator, Ticker> {
    
    @Override
    public TickerAccumulator createAccumulator() {
        return new TickerAccumulator();
    }

    @Override
    public TickerAccumulator add(Trade trade, TickerAccumulator acc) {
        // Accumulate volume
        acc.volume += trade.getVolume();
        // Update high price
        if (trade.getPrice() > acc.high) {
            acc.high = trade.getPrice();
        }
        // Update low price
        if (acc.low == 0 || trade.getPrice() < acc.low) {
            acc.low = trade.getPrice();
        }
        // Record last price
        acc.lastPrice = trade.getPrice();
        return acc;
    }

    @Override
    public Ticker getResult(TickerAccumulator acc) {
        Ticker ticker = new Ticker();
        ticker.setVolume(acc.volume);
        ticker.setHigh(acc.high);
        ticker.setLow(acc.low);
        ticker.setLastPrice(acc.lastPrice);
        return ticker;
    }

    @Override
    public TickerAccumulator merge(TickerAccumulator a, TickerAccumulator b) {
        // Merge multiple accumulators (for parallel computation)
        a.volume += b.volume;
        a.high = Math.max(a.high, b.high);
        a.low = Math.min(a.low, b.low);
        return a;
    }
}
```

**Interview Points**:
- `AggregateFunction` is incremental aggregation, higher performance than full aggregation
- Calls `add()` on each data arrival, no need to cache all data
- `merge()` is used for merging partial results in distributed scenarios

---

## 3. Kafka Source Configuration Analysis

### 3.1 KafkaSource Construction (Inferred)
```java
public class KafkaSourceUtils {
    public static <T> KafkaSource<T> of(InputConfig config, Class<T> clazz) {
        return KafkaSource.<T>builder()
            // Kafka cluster address
            .setBootstrapServers(config.getKafkaBootstrapServers())
            
            // Subscribed Topic
            .setTopics(config.getTopic())
            
            // Consumer group ID
            .setGroupId(config.getGroupId())
            
            // Starting position (LATEST: latest data, EARLIEST: from beginning)
            .setStartingOffsets(OffsetsInitializer.latest())
            
            // Deserializer
            .setValueOnlyDeserializer(new JsonDeserializer<>(clazz))
            
            .build();
    }
}
```

### 3.2 Key Configuration Items
```java
public class InputConfig {
    private String kafkaBootstrapServers = "localhost:9092";
    private String topic;
    private String groupId;
    private String checkpointPath;
    private int parallelism = 1;
    
    public static InputConfig build(String topic, String groupId) {
        InputConfig config = new InputConfig();
        config.setTopic(topic);
        config.setGroupId(groupId);
        return config;
    }
}
```

**Interview Points**:
- `topic`: Subscribed Kafka topic (e.g., order, trade)
- `groupId`: Consumer group ID for Offset management
- `parallelism`: Parallelism, controls resource usage

---

## 4. Redis Sink Implementation

### 4.1 OrderBookSink (Inferred Implementation)
```java
public class OrderBookSink extends RichSinkFunction<OrderBook> {
    private transient RedisClient redisClient;

    @Override
    public void open(Configuration parameters) throws Exception {
        // Initialize Redis connection
        redisClient = RedisClient.getInstance();
    }

    @Override
    public void invoke(OrderBook orderBook, Context context) throws Exception {
        String key = String.format("orderbook:%d:%d", 
            orderBook.getResolution(), 
            orderBook.getContractId());
        
        // Serialize to JSON
        String value = JSON.toJSONString(orderBook);
        
        // Write to Redis
        redisClient.set(key, value);
        
        // Optional: set expiration time
        redisClient.expire(key, 60);  // 60 seconds expiration
    }

    @Override
    public void close() throws Exception {
        if (redisClient != null) {
            redisClient.close();
        }
    }
}
```

### 4.2 Async Sink Optimization (Advanced)
```java
public class AsyncOrderBookSink extends RichAsyncFunction<OrderBook, Void> {
    private transient RedisAsyncClient redisClient;

    @Override
    public void asyncInvoke(OrderBook orderBook, ResultFuture<Void> resultFuture) 
        throws Exception {
        
        // Async write to Redis
        CompletableFuture<String> future = redisClient.setAsync(key, value);
        
        // Callback handling
        future.whenComplete((result, error) -> {
            if (error != null) {
                resultFuture.completeExceptionally(error);
            } else {
                resultFuture.complete(Collections.emptyList());
            }
        });
    }
}
```

**Interview Points**:
- Sync Sink: Simple but blocking, suitable for low throughput scenarios
- Async Sink: High throughput, but complex implementation
- Can configure retry strategy for network jitter

---

## 5. market-outer Module Code Analysis

### 5.1 Spring Boot Startup Class
```java
@EnableScheduling
@SpringBootApplication
public class MarketOuterApplication {
    public static void main(String[] args) {
        // Initialize Redis client
        RedisClient.init("localhost", 6379, "");
        
        // Start Spring Boot
        SpringApplication.run(MarketOuterApplication.class, args);
    }
}
```

**Interview Points**:
- `@EnableScheduling`: Enable scheduled tasks
- Redis initialized before Spring startup, available for other components

### 5.2 WebSocket Client (Inferred)
```java
public class CryptoSocketClient extends BaseSocketClient {
    
    @Override
    public void onOpen(ServerHandshake handshake) {
        log.info("WebSocket connected");
        
        // Send subscription request
        CryptoRequest request = new CryptoRequest();
        request.setMethod("subscribe");
        request.setParams(Arrays.asList("book.BTC_USDT"));
        
        send(JSON.toJSONString(request));
    }

    @Override
    public void onMessage(String message) {
        // Parse message
        CryptoEvent event = JSON.parseObject(message, CryptoEvent.class);
        
        // Write to Redis or Kafka
        if ("book".equals(event.getChannel())) {
            redisWriter.writeOrderBook(event.getData());
        }
    }

    @Override
    public void onError(Exception ex) {
        log.error("WebSocket error", ex);
    }

    @Override
    public void onClose(int code, String reason, boolean remote) {
        log.warn("WebSocket closed: {}", reason);
        // Reconnection logic
        scheduleReconnect();
    }
}
```

**Interview Points**:
- Extends `WebSocketClient` to implement custom logic
- Heartbeat keep-alive: periodically send ping messages
- Disconnect reconnection: exponential backoff strategy (1s ‚Üí 2s ‚Üí 4s ‚Üí 8s)

---

## 6. Key Data Models

### 6.1 Order
```java
public class Order {
    private Long contractId;      // Contract ID
    private String side;           // BID/ASK
    private String action;         // INSERT/DELETE
    private BigDecimal price;      // Price
    private BigDecimal quantity;   // Quantity
    private Long timestamp;        // Timestamp
}
```

### 6.2 OrderBook
```java
public class OrderBook {
    private Long contractId;
    private Map<BigDecimal, BigDecimal> bids;  // Bids
    private Map<BigDecimal, BigDecimal> asks;  // Asks
    private Double grouping;                   // Grouping granularity
    private Market market;
}
```

### 6.3 Trade
```java
public class Trade {
    private Long contractId;
    private BigDecimal price;
    private BigDecimal volume;
    private Long timestamp;
}
```

---

## 7. Common Interview Code Questions

### Q1: Why use BigDecimal instead of Double?
**A**:
```java
// Double precision issue
double a = 0.1 + 0.2;  // 0.30000000000000004

// BigDecimal precise calculation
BigDecimal a = new BigDecimal("0.1");
BigDecimal b = new BigDecimal("0.2");
BigDecimal c = a.add(b);  // 0.3
```
Financial scenarios must use BigDecimal to ensure calculation precision.

### Q2: How to ensure exactly-once semantics in Flink jobs?
**A**:
1. Enable Checkpoint
2. Kafka Source configured with `ENABLE_AUTO_COMMIT=false`
3. Sink implements idempotent writes or transactional writes
4. Code already uses KeyedProcessFunction + State, automatically supported

### Q3: What if Kafka messages accumulate?
**A**:
1. Increase parallelism (`setParallelism()`)
2. Optimize processing logic, reduce computation time
3. Increase Kafka partitions
4. Use Flink's Backpressure mechanism for rate limiting

### Q4: How to monitor Flink job health status?
**A**:
1. Flink Web UI (port 8081)
2. Flink Metrics (JMX, Prometheus)
3. Custom Metrics (e.g., processing latency, failure rate)
4. Log monitoring (ELK)

---

## 8. Code Optimization Suggestions (Bonus Points)

### 8.1 Add Data Deduplication
```java
// Use Flink State for deduplication
public class DeduplicateProcessor 
    extends KeyedProcessFunction<Long, Order, Order> {
    
    private ValueState<Long> lastSeenIdState;

    @Override
    public void processElement(Order order, Context ctx, Collector<Order> out) 
        throws Exception {
        Long lastSeenId = lastSeenIdState.value();
        if (lastSeenId == null || order.getId() > lastSeenId) {
            lastSeenIdState.update(order.getId());
            out.collect(order);
        } else {
            log.warn("Duplicate order: {}", order.getId());
        }
    }
}
```

### 8.2 Add Latency Monitoring
```java
public class OrderBookProcessor 
    extends KeyedProcessFunction<Long, Order, OrderBook> {
    
    private transient Counter lateCounter;

    @Override
    public void open(Configuration parameters) throws Exception {
        lateCounter = getRuntimeContext()
            .getMetricGroup()
            .counter("late_orders");
        // ... other initialization
    }

    @Override
    public void processElement(Order order, Context ctx, Collector<OrderBook> out) 
        throws Exception {
        long delay = System.currentTimeMillis() - order.getTimestamp();
        if (delay > 5000) {
            lateCounter.inc();
            log.warn("Late order: {}ms", delay);
        }
        // ... processing logic
    }
}
```

---

**This concludes the core code deep dive. Recommend repeatedly studying with actual code!**
